---
layout: post
title: 两种时序数据库的实现思路及选型
author: zhangyue
---

## TSDB的分类

* 传统TSDB，不支持实时计算
    * 特点：什么样子存进去，基本是什么样子取出来
    * 实现思路： 利用顺序读来提升性能
    * 挑战
        * 如何尽可能的缩减成本
    * 缺点
        * 任何数据聚合计算都需要上游预先计算完成
        * 维度交叉组合造成维度爆炸
    * 典型实现
        * 基于简单的框架：基于RDD-Tools的Open-falcon、基于Whisper的Graphite 
        * 基于Kv引擎： 基于Hbase的OpenTSDB、基于levelDB的promethus、基于cassandra的kairosDB
* 新型TSDB，支持实时计算
    * 特点：不仅可以直接取出，还可以在查询阶段对数据进行任意聚合运算
    * 实现思路：将数据加载到内存中即时计算： 
    * 挑战
        * 如何从数十亿数据中滤出数百万的目标数据
            * 基于倒排索引的高效数据滤出
        * 如何高效的将这不连续的数百万数据加载到内存（克服随机读的性能问题）
            * 依靠列存储的压缩特性，可利用mmap映射，从而支持高效的随机读。
        * 如何对这数百万数据进行聚合计算
            * 高度并行和分布式带来的高性能，理论上可以利用集群所有的CPU和内存来进行计算，基于MapReduce思想
            * 二阶段聚合：每个shard在本地计算自己的数据， 然后将结果发送到一处进行合并计算
            * 优势
                * 带宽方面：shard在本地计算本shard的聚合结果，不需要传输全量数据。
                * 内存方面：shard只将计算结果传递给condinator，因此condinator的内存不存在瓶颈。
    * 典型实现：
        * ElasticSearch
        * Druid.io
        
## 两种类型的技术思路对比

以ES和OpenTSDB举例
* OpenTSDB基于HBase，读取效率严重依赖于顺序性。其Rowkey规则为 [metric][timestamp][tag] , 因此只适合单个metric的时间顺序查找，对于数据加载到内存，也只有顺序读取单个metric时才能避免随机IO。OpenTSDB的聚合过程是一阶段过程，其将原始数据从HBase分区中统一缓存在condinator的内存中再进行单机计算，会造成网络压力，以及受到condinator内存大小和cpu速度的限制。
* ES基于搜索引擎，写入依赖顺序性，但检索无法依靠顺序性加速。其检索是Query And Fetch 二阶段检索。首先利用倒排快速找到任意位置的数据，之后利用列存储和mmap实现对随机读的优化来加载数据。最后利用本地计算和只传输结果给聚合节点，来节省资源。

## 选型思路

* 如果事先已经想好数据将要如何聚合和使用，或数据维度小但频次大，则使用传统TSDB配合前置流计算的形式。例如基础监控中对机器指标的监控。
* 如果事先不知道数据将如何聚合和使用，或维度过多，则应使用实时计算TSDB。例如对用户行为、业务流转等数据的探索分析。
* 结合使用： 用支持实时计算的TSDB，同时也有前置流计算引擎对已知如何使用或量很大的数据进行预计算，减少实时计算的频次。